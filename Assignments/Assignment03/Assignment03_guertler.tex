\documentclass[
	10pt, % Default font size, values between 10pt-12pt are allowed
]{../fphw}

% Template-specific packages
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font
\usepackage{graphicx} % Required for including images
\usepackage{booktabs} % Required for better horizontal rules in tables
\usepackage{listings} % Required for insertion of code
\usepackage{enumerate} % To modify the enumerate environment

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Probability Theory Review} % Assignment title
\author{Fabienne Guertler 12-935-508} % Student name
\date{October 9th, 2019} % Due date
\institute{University of Bern} % Institute or school name
\class{Machine Learning} % Course or class name

\begin{document}
	\maketitle
	\section*{Question 1 (15 points)}
	\begin{problem}
		Show that the covariance matrix is always symmetric and positive semidefinite.
	\end{problem}
	\paragraph{Solution:} A matrix A is positive semidefinite if $x^TAx \ge 0$ for all $x\in\mathbb{R}^n$.\\
	Covariance matrix $\Sigma = cov[X,X] = E[XX^T] + E[X]E[X]^T$ with $X\in\mathbb{R}^n$. Using $Cov[X,X] = Var[X]$ we get
	\begin{align*}
		u^TVar[X]u = Var[uX] 
	\end{align*}
	Because variance is always non-negative, we get $u^TVar[X]u \ge 0$.
	A matrix that is positive-semidefinite is automatically symmetric.
	
	\section*{Question 2 (20 points)}
	\begin{problem}
		$X\in R^n\text{ and }Y\in R^m$ are independent random variables. Their expectations and covariances are $E[X] = 0, Cov[X] = I, E[Y] = \mu\text{, and } Cov[Y]=\sigma I$, where $I$ is the identity matrix of the appropriate size and $\sigma$ is a scalar. What is the expectation and covariance of the random variable $Z = AX + Y$ , where $A\in R^{m\times n}$?
	\end{problem}
	\paragraph{Solution:} Expectation for random variable Z is
	\begin{equation*}
		E[Z] = E[AX+Y] = AE[X]+E[Y] = A\cdot 0 + \mu = \mu
	\end{equation*}
	the variance of Z
	\begin{equation*}
		Var[Z] = Var[AX+Y] = Var[AX] + Var[Y] + 2Cov[X,Y]
	\end{equation*}
	Because $X,Y$ are independent random variables $Cov[X,Y]$ is 0. This leads to
	\begin{equation*}
		Var[Z] = Var[AX] + Var[Y] = AVar[X]A^T + Var[Y] = AIA^T + \sigma I = I(A^TA + \sigma)
	\end{equation*}

	\section*{Question 3 (15 points)}
	\begin{problem}
		Thomas and Viktor are friends. It is Friday night and Thomas does not have a phone. Viktor knows that there is a 2/3 probability
		that Thomas goes to the party to downtown. There are 5 pubs in downtown and there is an equal probability of Thomas going to
		any of them if he goes to the party. Viktor already looked for Thomas in 4 of the bars.
		What is the probability of Viktor finding Thomas in the last bar?
	\end{problem}

	\section*{Question 4 (20 points)}
	\begin{problem}
		Derive the mean for the Beta Distribution, which is defined as
		\begin{equation*}
			Beta(x|a,b)=\frac{1}{B(a,b)} x^{a-1}(1-x)^{b-1} 
		\end{equation*}
		where $B(a, b) , \Gamma(a)$ are Beta and Gamma functions respectively:
		\begin{equation*}
			B(a, b) = \frac{\Gamma (a)\Gamma (b)}{\Gamma (a+b)}
		\end{equation*}
		\begin{equation*}
			\Gamma(x) = \int_{0}^{\infty}u^{x-1}e^{-u}du
		\end{equation*}
	\end{problem}
	\paragraph{Solution:} To get integral representation of the beta function
	\begin{align*}
		B(a,b)\Gamma(a+b) &= \Gamma(a)\Gamma(b)\\
		&= \int_{0}^{\infty}e^{-u}u^{a-1}du \cdot \int_{0}^{\infty}e^{-v}v^{b-1}dv\\
		&= \int_{v=0}^{\infty}\int_{u=0}^{\infty}e^{-u-v}u^{a-1}v^{b-1}dudv\\
		u=f(z,t)=zt,\quad v=g(z,t)=z(1-t)\\
		&= \int_{z=0}^{\infty}\int_{t=0}^{\infty} e^{-z}(zt)^{a-1}(z(1-t))^{b-1}|J(z,t)|dtdz\\
		&= \int_{z=0}^{\infty}\int_{t=0}^{\infty} e^{-z}(zt)^{a-1}(z(1-t))^{b-1}zdtdz\\
		&= \int_{z=0}^{\infty}e^{-z}z^{a+b-1}dz \cdot \int_{t=0}^{1}t^{a-1}(1-t)^{b-1}dt
	\end{align*}
	We know that $\Gamma(a+b)=\int_{0}^{\infty}e^{-z}z^{a+b-1}dz$ so we get
	\begin{equation*}
		B(a,b) = \int_{0}^{1}t^{a-1}(1-t)^{b-1}dt
	\end{equation*}
	The mean of beta distribution
	\begin{align*}
		E(x) &= \int_{0}^{1}xBeta(x|a,b)dx\\
		&= \int_{0}^{1}x\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}dx\\
		&= \frac{1}{B(a,b)}\int_{0}^{1}x^a(1-x)^{b-1}dx\\
		&= \frac{B(a+1,b)}{B(a,b)} 
		= \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} 
		= \frac{a\Gamma(a)\Gamma(b)\Gamma(a+b)} {(a+b)\Gamma(a+b)\Gamma(a)\Gamma(b)} 
		= \frac{a}{a+b}
	\end{align*}
	
	\section*{Question 5 (20 points)}
	\begin{problem}
		Let $A\in\mathbb{R}^{n\times n}$ be a positive definite square matrix, $b\in\mathbb{R}^n$, and $c$ be a scalar. Prove that
		\begin{equation*}
			\int_{x\in\mathbb{R}^n} e^{-\frac{1}{2}x^TAx-x^Tb-c} dx 
			= \frac{(2\pi)^{n/2}|A|^{-1/2}}{e^{c-\frac{1}{2}b^TA^{-1}b}}
		\end{equation*}
	\end{problem}
	\paragraph{Solution:} We know that the integral of the Gaussian probability density function on a random variable with mean $\mu$ and covariance $\Sigma$ is 1.
	\begin{equation*}
		\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}\int_{x\in\mathbb{R}^n}\exp(-\frac{1}{2} (x-\mu)^T\Sigma^{-1}(x-\mu)) = 1
	\end{equation*}
	\begin{equation*}
		-\frac{1}{2}x^TAx - x^Tb - c = -\frac{1}{2}(x+A^{-1}b)^TA(x+A^{-1}b) - c + \frac{1}{2}b^TA^{-1}b
	\end{equation*}
	We can factor out the terms not including x
	\begin{align*}
		\int\exp(-\frac{1}{2}x^TAx-x^Tb-c)dx &= \int\exp(-\frac{1}{2}(x+A^{-1}b)^TA(x+A^{-1}b) - c + \frac{1}{2}b^TA^{-1}b) dx\\
		&= \exp(- c + \frac{1}{2}b^TA^{-1}b) \cdot \int\exp(-\frac{1}{2}(x+A^{-1}b)^TA(x+A^{-1}b))dx
	\end{align*}
	We use $\int\exp(-\frac{1}{2} (x-\mu)^T\Sigma^{-1}(x-\mu))dx = (2\pi)^{n/2}|\Sigma|^{1/2}$ and $\Sigma = A^{-1}$ to get rid of the remaining integral and get
	\begin{equation*}
		\int\exp(-\frac{1}{2}x^TAx-x^Tb-c)dx = \frac{(2\pi)^{n/2}|A|^{-1/2}}{e^{c-\frac{1}{2}b^TA^{-1}b}}
	\end{equation*}

	\section*{Question 6 (10 points)}
	\begin{problem}
		From the definition of conditional probability of multiple random variables, show that
		\begin{equation*}
			f(x_1,x_2,...,x_n) = f(x_1)\prod_{i=2}^{n}f(x_i|x_1,...,x_n)
		\end{equation*}
		where $x_1,x_2,...,x_n$ are random variables and f is a probability density function of its arguments.
	\end{problem}
	\paragraph{Solution:} The conditional probability density of multiple random variables
	\begin{equation*}
		f_{X_n|X_1,...,X_{n-1}}(x_n|x_1,...,x_{n-1}) = \frac{f_{X_1,X_2,...,X_n}(x_1,x_2,...,x_n)}{f_{X_1,...,X_{n-1}}(x_1,x_2,...,x_n)}
	\end{equation*}
	Solve the equation for $f(x_1,...x_n)$
	\begin{align*}
		f(x_1,x_2,...,x_n) &= f(x_n|x_1,...,x_{n-1})f(x_1,...,x_{n-1})\\
		&= f(x_n|x_1,...,x_{n-1})f(x_n|x_1,...,x_{n-2})f(x_1,...,x_{n-2})\\
		&= f(x_n|x_1,...,x_{n-1})f(x_n|x_1,...,x_{n-2})f(x_1,...,x_{n-2})\cdot ...\cdot f(x2|x_1)f(x1)\\
		&= f(x_1)\prod_{i=2}^{n}f(xi|x1,...,x_{i-1})
	\end{align*}
	
\end{document}
